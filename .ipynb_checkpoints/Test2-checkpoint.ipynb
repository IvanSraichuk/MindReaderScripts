{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc872669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import E\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import neurokit2 as nk2\n",
    "import neurokit as nk\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import axis\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import wfdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29918f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify(name,model,features,classes):\n",
    "    \n",
    "    #Using 10-KFold cross validation\n",
    "    kf = KFold(10,shuffle=True)\n",
    "    score = []\n",
    "    for trainIndex,testIndex in kf.split(features):\n",
    "        xTrain, xTest = features[trainIndex],features[testIndex]\n",
    "        yTrain, yTest = classes[trainIndex],classes[testIndex]\n",
    "        \n",
    "        model.fit(xTrain,yTrain)\n",
    "        prediction = model.predict(xTest)\n",
    "        score.append( accuracy_score(yTest,prediction))\n",
    "        \n",
    "        \n",
    "    print(\"The average score for \",name,\" is \",sum(score)/len(score))\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocessClass(classes,name):\n",
    "    \n",
    "    for idx,val in enumerate(classes):\n",
    "        \n",
    "        if val == name:\n",
    "            classes[idx] = '0'\n",
    "        else:\n",
    "            classes[idx] = '1'\n",
    "    \n",
    "    return classes\n",
    "    \n",
    "def classificationProcess(features,classes):\n",
    "    \n",
    "    #Calculate the Feature Importance using Forest Tree\n",
    "    model = ExtraTreesClassifier(100)\n",
    "    model.fit(features,classes)\n",
    "    featureImportance = model.feature_importances_\n",
    "    models = dict()\n",
    "    specificClass = 'A'\n",
    "    \n",
    "    #if specificClass != 'all':\n",
    "    classes = preprocessClass(classes, specificClass)\n",
    "    \n",
    "    #Choose the 5 features with the highest importance\n",
    "    bestFeatures= []\n",
    "    for i in range(6):\n",
    "        max = featureImportance.argmax()\n",
    "        bestFeatures.append(max)\n",
    "        featureImportance[max] = 0\n",
    "     \n",
    "     #Adjust the features array  to contain only the columns of the best features   \n",
    "    features = features[:,[bestFeatures[0],bestFeatures[1],bestFeatures[2],bestFeatures[3],bestFeatures[4]]]\n",
    "    \n",
    "    ######KNN###########\n",
    "    parameters = []\n",
    "    knn = KNeighborsClassifier(4)\n",
    "    models[\"KNN\"] = classify(\"KNN\",knn, features, classes)\n",
    "    \n",
    "    #Using exhaustive GridSearchCV for parameter tuning \n",
    "    \n",
    "    #####SVM############\n",
    "    parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]}]\n",
    "    svm = GridSearchCV(SVC(),parameters, cv=5)\n",
    "    models[\"SVM\"] = classify(\"SVM\",svm,features,classes)\n",
    "    models[\"ExtraTreesClassifier\"] = classify(\"SVM\",model,features,classes)\n",
    "    #####Random Forest #####\n",
    "    parameters = [{'n_estimators': [100, 500, 1000,1500]}]\n",
    "    forest = GridSearchCV(RandomForestClassifier(),parameters, cv=5)\n",
    "    models[\"RF\"] = classify(\"Random Forest\",forest,features,classes)\n",
    "    \n",
    "    #####Naive Bayes ######\n",
    "    gauss = GaussianNB()\n",
    "    models[\"GAUS\"] = classify(\"Gaussian\", gauss, features, classes)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0de59996",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "classes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07340e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_number = '05091'\n",
    "filepath = 'D:/phd/mit-bih-atrial-fibrillation-database-1.0.0/files/' + record_number\n",
    "record = wfdb.rdrecord(filepath)\n",
    "annotation = wfdb.rdann(filepath,'atr', shift_samps=True)\n",
    "\n",
    "#writeToCsv(features,classes)\n",
    "\n",
    "#features, classes = loadFromFile()\n",
    "#models = classificationProcess(features,classes)\n",
    "points = [79, 1633050, 1633999, 1900120, 1901525, 2270154, 2270587,\n",
    "    2574098, 2575190, 2591265, 2593950, 2618884, 2619973, 2877400,\n",
    "    2880801, 4350349, 4360987]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c91e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessSignal(signal, rate):\n",
    "    peaks = nk2.ecg_findpeaks(signal, rate)\n",
    "     #Calculate the Heart Rate Variability and extract its measures\n",
    "    hrv = nk2.hrv(peaks, sampling_rate=rate)\n",
    "    \n",
    "    #Calculate the features\n",
    "    mean = signal.mean() # Mean of the ECG signal\n",
    "    std = signal.std() # Standard deviation of the ECG signal\n",
    "    sdNN = list(hrv['HRV_SDNN'])[0] # Standard deviation of RR interval\n",
    "    meanNN = list(hrv['HRV_MeanNN'])[0] # Mean of RR interval \n",
    "    madNN = list(hrv['HRV_MadNN'])[0]  # Median Absolute Deviation (MAD) of the RR intervals.\n",
    "    pNN50 = list(hrv['HRV_pNN50'])[0] # The number of interval differences of successive RR intervals greater than 50 ms\n",
    "    RMSSD = list(hrv['HRV_RMSSD'])[0] # the root mean square of the RR intervals\n",
    "    shannon = list(hrv['HRV_ShanEn'])[0] # Shannon entropy\n",
    "    sample_entropy = list(hrv['HRV_SampEn'])[0] # Sample Entropy\n",
    "    if np.isinf(sample_entropy):\n",
    "        sample_entropy = 0\n",
    "    return [mean, std, sdNN, meanNN, madNN, pNN50, RMSSD, shannon, sample_entropy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "018777e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1633050\n",
      "1900120\n",
      "2270154\n",
      "2574098\n",
      "2591265\n",
      "2618884\n",
      "2877400\n",
      "4350349\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, int(len(record.p_signal)), 10000):\n",
    "        signal = record.p_signal[i : i +10000, 0]\n",
    "        class_signal = 'N'\n",
    "        for j in range(len(points)):\n",
    "            if j %2 == 0:\n",
    "                continue\n",
    "            if points[j] > i and points[j] < i + 10000:\n",
    "                print(points[j])\n",
    "                class_signal = 'A'\n",
    "        feature = ProcessSignal(signal, 250)\n",
    "        features.append(feature)\n",
    "        classes.append(class_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e45cecbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(921, 921)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features), len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5baafaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score for  KNN  is  0.9905016495817133\n",
      "The average score for  SVM  is  0.9951138505950278\n",
      "The average score for  SVM  is  0.9913124484505715\n",
      "The average score for  Random Forest  is  0.9910458642629905\n",
      "The average score for  Gaussian  is  0.9611810710498409\n"
     ]
    }
   ],
   "source": [
    "models = classificationProcess(np.array(features), np.array(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3694e27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.75858172e-01, 2.20371980e+01],\n",
       "       [1.79547221e-01, 2.22473233e+01],\n",
       "       [1.76467709e-01, 2.05104746e+01],\n",
       "       ...,\n",
       "       [3.95906881e-01, 9.65447896e+01],\n",
       "       [8.10898198e-01, 2.01067891e+02],\n",
       "       [2.08851883e+00, 2.57362313e+02]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(features)[: , [1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "26785834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KNN': KNeighborsClassifier(n_neighbors=4),\n",
       " 'SVM': GridSearchCV(cv=5, estimator=SVC(),\n",
       "              param_grid=[{'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001],\n",
       "                           'kernel': ['rbf']}]),\n",
       " 'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
       " 'RF': GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "              param_grid=[{'n_estimators': [100, 500, 1000, 1500]}]),\n",
       " 'GAUS': GaussianNB()}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd4ec390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the Feature Importance using Forest Tree\n",
    "model = ExtraTreesClassifier(100)\n",
    "model.fit(features,classes)\n",
    "featureImportance = model.feature_importances_\n",
    "#Choose the 5 features with the highest importance\n",
    "bestFeatures= []\n",
    "for i in range(6):\n",
    "    maxМ = featureImportance.argmax()\n",
    "    bestFeatures.append(maxМ)\n",
    "    featureImportance[maxМ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b2b3a839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN ['0']\n",
      "SVM ['1']\n",
      "ExtraTreesClassifier ['1']\n",
      "RF ['1']\n",
      "GAUS ['1']\n"
     ]
    }
   ],
   "source": [
    "p = 2270154+1000\n",
    "signal = record.p_signal[p : p +10000, 0]\n",
    "r = ProcessSignal(signal, 250)\n",
    "t = []\n",
    "t.append(r[bestFeatures[0]])\n",
    "t.append(r[bestFeatures[1]])\n",
    "t.append(r[bestFeatures[2]])\n",
    "t.append(r[bestFeatures[3]])\n",
    "t.append(r[bestFeatures[4]])\n",
    "for i in models.items():\n",
    "    name, model = i\n",
    "    print(name, model.predict(np.reshape(t, (1, -1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a2b1788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.194385682587765"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from re import M\n",
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import joblib\n",
    "\n",
    "import pickle\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from os import walk\n",
    "import os\n",
    "import csv\n",
    "import pdb\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "path = 'D:\\ECGAttempts\\Atrial-Fibrilation-Detection\\training2017\\'\n",
    "\n",
    "batch_size = 10\n",
    "epochs = 20\n",
    "num_classes = 10\n",
    "no_samples = 18286\n",
    "no_files = 8528\n",
    "dim_maxima = 18286\n",
    "\n",
    "\n",
    "def test ():\n",
    "\n",
    "    test = np.load(\"/home/iustin/python code/testingset.npy\")\n",
    "    etichete_test = np.load (\"/home/iustin/python code/testinglabels.npy\")\n",
    "\n",
    "    model = pickle.load(open(\"model.pkl\", \"rb\"))\n",
    "    model.predict(test)\n",
    "\n",
    "    _, accuracy = model.evaluate(test, etichete_test, batch_size=batch_size, verbose = 1)\n",
    "    print (\"ACCURACY FOR TEST... \")\n",
    "    print (accuracy)\n",
    "\n",
    "\n",
    "def label_processing():\n",
    "\n",
    "    # noua baza de date cu etichetele 1 si O\n",
    "    d = []\n",
    "\n",
    "    with open(path + 'REFERENCE.csv') as csvfile:\n",
    "        content = csv.reader (csvfile, delimiter = ',')\n",
    "        for c in content:\n",
    "            if c[1] == 'A':\n",
    "                c[1] = 1\n",
    "            else :\n",
    "                c[1] = 0\n",
    "            d.append(c)\n",
    "\n",
    "    with open('db.csv', 'w', newline='') as csvfile:\n",
    "        s = csv.writer(csvfile, delimiter = ',')\n",
    "        for r in d:\n",
    "            s.writerow (r)\n",
    "\n",
    "\n",
    "def citire_date():\n",
    "\n",
    "    # citesc datele din fisier\n",
    "    d = {}\n",
    "\n",
    "    #csv contine nume imagine si label\n",
    "    with open('/home/iustin/python code/db.csv', 'r', newline='') as csvfile:\n",
    "        content = csv.reader (csvfile, delimiter = ',')\n",
    "        for c in content:\n",
    "            d [c[0]] = c[1]\n",
    "    \n",
    "    p1 = int (0.9 * no_files)\n",
    "    p2 = int (0.7 * p1)\n",
    "\n",
    "    dtrain = {}\n",
    "    dtest = {}\n",
    "    dvalidation = {}\n",
    "\n",
    "    for cheie, val in d.items():\n",
    "        if p2 > 0:\n",
    "            dtrain [path + cheie] = val\n",
    "            p2 -= 1\n",
    "            p1 -= 1\n",
    "        elif p1 > 0:\n",
    "            dvalidation [path + cheie] = val\n",
    "            p1 -= 1\n",
    "        else :\n",
    "            dtest [path + cheie] = val\n",
    "    return dtrain, dvalidation, dtest\n",
    "\n",
    "\n",
    "def preprocesare_train():\n",
    "\n",
    "    dtrain, _, _ = citire_date() \n",
    "    train = []\n",
    "    etichete_train = np.array ([])\n",
    "    contor = 0\n",
    "    print (\"Incarc etichetele si datele in memorie ....\")\n",
    "    print (\"Start training set\")\n",
    "\n",
    "    #cheia este calea catre imagine, val este labelul\n",
    "    for cheie, val in dtrain.items():\n",
    "        mat = scipy.io.loadmat(cheie)\n",
    "        #incarcarea continutului fisierului din cheie\n",
    "        #pentru semnalele cu lungimea mai mica de dim maxima, se adauga elemente de 0 pana ce dimensiunea este egala cu dim maxima\n",
    "        if len(mat['val'][0]) < dim_maxima :\n",
    "            pad = [float(0)] * (dim_maxima - len(mat['val'][0]))\n",
    "            train.append([float(x) for x in mat['val'][0]] + pad)\n",
    "        else :\n",
    "            train.append([float(x) for x in mat['val'][0]])\n",
    "        etichete_train = np.append(etichete_train, float(val))\n",
    "        print (\"Done processing Sample {} + Label {} ...\".format(contor, contor))\n",
    "        contor += 1\n",
    "\n",
    "    print (\"Done traning set\")\n",
    "    print (\"START - saving train set\")\n",
    "    np.save(\"/home/iustin/python code/trainlabels\", np.array(train))\n",
    "    print (\"END - saving train set\")\n",
    "    print (\"START - saving train labels\")\n",
    "    np.save(\"/home/iustin/python code/trainlabels\", etichete_train)\n",
    "    print (\"END - saving train labels\")\n",
    "\n",
    "\n",
    "def preprocesare_validation():\n",
    "\n",
    "    _, dvalidation, _ =  citire_date() \n",
    "    validation = []\n",
    "    etichete_validation = np.array ([])\n",
    "    contor = 0\n",
    "    print (\"Start validation set\")\n",
    "\n",
    "    for cheie, val in dvalidation.items():\n",
    "        mat = scipy.io.loadmat(cheie)\n",
    "        if len(mat['val'][0]) < dim_maxima :\n",
    "            pad = [0] * (dim_maxima - len(mat['val'][0]))\n",
    "            validation.append(list(mat['val'][0]) + pad)\n",
    "        else :\n",
    "            validation.append(list(mat['val'][0]))\n",
    "        etichete_validation = np.append(etichete_validation, float(val))\n",
    "        print (\"Done processing Sample {} + Label {} ...\".format(contor, contor))\n",
    "        contor += 1\n",
    "\n",
    "    print (\"Done validation set\")\n",
    "    print (\"START - saving validation set\")\n",
    "    np.save(\"/home/iustin/python code/validationset\", np.array(validation))\n",
    "    print (\"END - saving validation set\")\n",
    "    print (\"START - saving validation labels\")\n",
    "    np.save(\"/home/iustin/python code/validationslabels\", np.array(etichete_validation))\n",
    "    print (\"END - saving validation labels\")\n",
    "\n",
    "\n",
    "def preprocesare_test():\n",
    "\n",
    "    _, _, dtest =  citire_date() \n",
    "    test = []\n",
    "    etichete_test = np.array ([])\n",
    "    contor = 0\n",
    "    print (\"Start test set\")\n",
    "\n",
    "    for cheie, val in dtest.items():\n",
    "        mat = scipy.io.loadmat(cheie)\n",
    "        if len(mat['val'][0]) < dim_maxima :\n",
    "            pad = [0] * (dim_maxima - len(mat['val'][0]))\n",
    "            test.append(list(mat['val'][0]) + pad)\n",
    "        else :\n",
    "            test.append(list(mat['val'][0]))\n",
    "        etichete_test = np.append(etichete_test, float(val))\n",
    "        print (\"Done processing Sample {} + Label {} ...\".format(contor, contor))\n",
    "        contor += 1\n",
    "\n",
    "    print (\"Done test set\")\n",
    "    print (\"START - saving testing set\")\n",
    "    np.save(\"/home/iustin/python code/testingset\", np.array(test))\n",
    "    print (\"END - saving testing set\")\n",
    "    print (\"START - saving testing labels\")\n",
    "    np.save(\"/home/iustin/python code/testinglabels\", np.array(etichete_test))\n",
    "    print (\"END - saving testing labels\")\n",
    "\n",
    "\n",
    "def cnn ():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(27, 1, padding='causal', input_shape=(no_samples, 1), activation=\"relu\"))\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv1D(15, 1, padding='causal', activation=\"relu\"))\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv1D(4, 1, padding='causal', activation=\"relu\"))\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv1D(3, 1, padding='causal', activation=\"relu\"))\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(30, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    " \n",
    "    # preprocesare date\n",
    "    # preprocesare_train()\n",
    "    # preprocesare_validation()\n",
    "    # preprocesare_test()\n",
    "\n",
    "    print (\"Creez si compilez cnn\")\n",
    "    model = cnn()\n",
    "    model.compile(\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        optimizer = 'adam',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    model.summary()\n",
    " \n",
    "\n",
    "    train = np.load(\"/home/iustin/python code/trainset.npy\")\n",
    "    validation = np.load(\"/home/iustin/python code/validationset.npy\")\n",
    "\n",
    "    etichete_train = np.load (\"/home/iustin/python code/trainlabels.npy\")\n",
    "    etichete_validation = np.load (\"/home/iustin/python code/validationslabels.npy\")\n",
    "\n",
    "    print (\"START TRAIN AND VALIDATION ... \")\n",
    "    model.fit(train, etichete_train, validation_data = (validation, etichete_validation), epochs=epochs, batch_size=batch_size, verbose = 1)\n",
    "    print (\"END\")\n",
    "\n",
    "    model = pickle.dump(model, open(\"model.pkl\", \"wb\"))\n",
    "    joblib.dump(model, 'modelsalvat.pkl')\n",
    "\n",
    "    # # test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75401771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
