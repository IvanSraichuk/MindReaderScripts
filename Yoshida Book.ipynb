{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b440c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Import Library\n",
    "#!pip install seaborn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import gc\n",
    "sns.set()\n",
    "sns.set_context('poster')\n",
    "\n",
    "# Setting Random Seed\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f5d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf410f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '..\\\\input\\\\1056lab-cardiac-arrhythmia-detection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     19\u001b[0m _input_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1056lab-cardiac-arrhythmia-detection\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_input_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m normal_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_input_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.mat\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(normal_files[: \u001b[38;5;241m10\u001b[39m])\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '..\\\\input\\\\1056lab-cardiac-arrhythmia-detection'"
     ]
    }
   ],
   "source": [
    "# TPU\n",
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "#\n",
    "_input_path = os.path.join('..', 'input', '1056lab-cardiac-arrhythmia-detection')\n",
    "os.listdir(_input_path)\n",
    "normal_files = sorted(glob(os.path.join(_input_path, 'normal', '*.mat')))\n",
    "print(normal_files[: 10])\n",
    "af_files = sorted(glob(os.path.join(_input_path, 'af', '*.mat')))\n",
    "print(af_files[: 10])\n",
    "test_files = sorted(glob(os.path.join(_input_path, 'test', '*.mat')))\n",
    "print(test_files[: 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "326248a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Datasets\n",
    "def load_data(pathes, label=None, prefix=None, max_length=0, min_length=np.inf, verbose=True):\n",
    "    verbose = not verbose  # tqdm用に反転\n",
    "\n",
    "    if prefix is not None:\n",
    "        if prefix.endswith(\"/\") or prefix.endswith(\"\\\\\"):\n",
    "            prefix += \"//\"\n",
    "\n",
    "        for i, val in enumerate(pathes):\n",
    "            pathes[i] = \"{}{}\".format(prefix, val)\n",
    "\n",
    "    data_array = []\n",
    "    labels = []\n",
    "    for i, f in enumerate(tqdm(pathes, disable=verbose)):\n",
    "        tmp = loadmat(f)\n",
    "        data_array.append(tmp[\"val\"].flatten())\n",
    "        tmp_len = len(data_array[i])\n",
    "\n",
    "        if max_length < tmp_len:\n",
    "            max_length = tmp_len\n",
    "        elif min_length > tmp_len:\n",
    "            min_length = tmp_len\n",
    "\n",
    "        if label is not None:\n",
    "            labels.append(label)\n",
    "\n",
    "    return data_array, labels, max_length, min_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1617861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Local Files.\n",
      "Max Length : 18286\n",
      "Min Length : 2714\n"
     ]
    }
   ],
   "source": [
    "dump_path = os.path.join(\"C:\\\\Users\\\\houfo\\\\Documents\\\\Kaggle\\\\yoshida146\", \"dump\")\n",
    "os.makedirs(dump_path, exist_ok=True)\n",
    "if \"normals.pkl\" in os.listdir(dump_path):\n",
    "    print(\"Load Local Files.\")\n",
    "\n",
    "    normals = joblib.load(os.path.join(dump_path, \"normals.pkl\"))\n",
    "    afs = joblib.load(os.path.join(dump_path, \"afs.pkl\"))\n",
    "    tests = joblib.load(os.path.join(dump_path, \"tests.pkl\"))\n",
    "    labels = joblib.load(os.path.join(dump_path, \"labels.pkl\"))\n",
    "\n",
    "    max_length = joblib.load(os.path.join(dump_path, \"max_length.pkl\"))\n",
    "    min_length = joblib.load(os.path.join(dump_path, \"min_length.pkl\"))\n",
    "else:\n",
    "    print(\"Load RAW Files.\")\n",
    "    normals, normal_label, max_length, min_length = load_data(normal_files, label=0)\n",
    "    afs, af_label, max_length, min_length = load_data(af_files, label=1, max_length=max_length, min_length=min_length)\n",
    "    tests, _, max_length, min_length = load_data(test_files, label=None, max_length=max_length, min_length=min_length)\n",
    "    labels = np.append(normal_label, af_label)\n",
    "\n",
    "    joblib.dump(normals, os.path.join(dump_path, \"normals.pkl\"))\n",
    "    joblib.dump(afs, os.path.join(dump_path, \"afs.pkl\"))\n",
    "    joblib.dump(tests, os.path.join(dump_path, \"tests.pkl\"))\n",
    "    joblib.dump(labels, os.path.join(dump_path, \"labels.pkl\"))\n",
    "\n",
    "    joblib.dump(max_length, os.path.join(dump_path, \"max_length.pkl\"))\n",
    "    joblib.dump(min_length, os.path.join(dump_path, \"min_length.pkl\"))\n",
    "print(\"Max Length : {}\\nMin Length : {}\".format(max_length, min_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65603f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triming Method : center\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 3551/3551 [00:00<00:00, 592252.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 533/533 [00:00<00:00, 211982.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1750/1750 [00:00<00:00, 347473.58it/s]\n"
     ]
    }
   ],
   "source": [
    "TRIM_METHOD = \"center\"\n",
    "\n",
    "print(\"Triming Method : {}\".format(TRIM_METHOD))\n",
    "if TRIM_METHOD == \"max\":\n",
    "    normals_ = normals.copy()\n",
    "    for i, v in enumerate(tqdm(normals_)):\n",
    "        diff = max_length - len(v)\n",
    "        #         normals_[i] = np.append(v, [v[-1]] * diff).reshape(-1, max_length)\n",
    "        normals_[i] = np.append([0] * diff, v).reshape(-1, max_length)\n",
    "\n",
    "    afs_ = afs.copy()\n",
    "    for i, v in enumerate(tqdm(afs_)):\n",
    "        diff = max_length - len(v)\n",
    "        #         afs_[i] = np.append(v, [v[-1]] * diff).reshape(-1, max_length)\n",
    "        afs_[i] = np.append([0] * diff, v).reshape(-1, max_length)\n",
    "\n",
    "    tests_ = tests.copy()\n",
    "    for i, v in enumerate(tqdm(tests_)):\n",
    "        diff = max_length - len(v)\n",
    "        #         tests_[i] = np.append(v, [v[-1]] * diff).reshape(-1, max_length)\n",
    "        tests_[i] = np.append([0] * diff, v).reshape(-1, max_length)\n",
    "\n",
    "elif TRIM_METHOD == \"min\":\n",
    "    normals_ = normals.copy()\n",
    "    for i, v in enumerate(tqdm(normals_)):\n",
    "        normals_[i] = v[: min_length]\n",
    "\n",
    "    afs_ = afs.copy()\n",
    "    for i, v in enumerate(tqdm(afs_)):\n",
    "        afs_[i] = v[: min_length]\n",
    "\n",
    "    tests_ = tests.copy()\n",
    "    for i, v in enumerate(tqdm(tests_)):\n",
    "        tests_[i] = v[: min_length]\n",
    "\n",
    "elif TRIM_METHOD == \"center\":\n",
    "    def clipping(x, n):\n",
    "        del_width = (len(x) - n) // 2\n",
    "        a_extracted = x[del_width: n + del_width]\n",
    "        return a_extracted\n",
    "\n",
    "\n",
    "    normals_ = normals.copy()\n",
    "    for i, v in enumerate(tqdm(normals_)):\n",
    "        normals_[i] = clipping(v, min_length)\n",
    "\n",
    "    afs_ = afs.copy()\n",
    "    for i, v in enumerate(tqdm(afs_)):\n",
    "        afs_[i] = clipping(v, min_length)\n",
    "\n",
    "    tests_ = tests.copy()\n",
    "    for i, v in enumerate(tqdm(tests_)):\n",
    "        tests_[i] = clipping(v, min_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8033b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataAugment():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _check_array(self, x):\n",
    "        return np.array(x) if type(x) == list else x\n",
    "\n",
    "    def add_noise(self, x, rate=0.05):\n",
    "        x = self._check_array(x)\n",
    "        x_ = x + rate * np.random.randn(len(x), 1)\n",
    "        x_ = x_.astype(np.float16)\n",
    "        return x_\n",
    "\n",
    "    def shift(self, x, rate=2):\n",
    "        x_ = np.roll(x, int(len(x) // rate))\n",
    "        x_ = x_.astype(np.float16)\n",
    "        return x_\n",
    "\n",
    "if TRIM_METHOD == \"max\":\n",
    "    normals_ = np.array(normals_).reshape(-1, max_length, 1)\n",
    "    afs_ = np.array(afs_).reshape(-1, max_length, 1)\n",
    "    tests_ = np.array(tests_).reshape(-1, max_length, 1)\n",
    "else:\n",
    "    normals_ = np.array(normals_).reshape(-1, min_length, 1)\n",
    "    afs_ = np.array(afs_).reshape(-1, min_length, 1)\n",
    "    tests_ = np.array(tests_).reshape(-1, min_length, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cc77e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, BatchNormalization, LSTM, Dropout, Conv1D, MaxPooling1D, SpatialDropout1D, Bidirectional, Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d17dedca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(X_shape, conv_activation=\"tanh\", conv_filters=64, dense_act=\"tanh\", optimizer=\"adam\"):\n",
    "    model = Sequential()\n",
    "\n",
    "    input_shape = X_shape[1], X_shape[2]\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Conv1D(conv_filters, 2, padding=\"same\", activation=conv_activation))\n",
    "    model.add(MaxPooling1D(2))\n",
    "\n",
    "    model.add(Conv1D(conv_filters, 2, padding=\"same\", activation=conv_activation))\n",
    "    model.add(MaxPooling1D(2))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(128, activation=\"tanh\", return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(64, activation=\"tanh\", return_sequences=False)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation=dense_act))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"AUC\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4030f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @profile\n",
    "def augment(data, label, N=10, verbose=True):\n",
    "    ada = AudioDataAugment()\n",
    "\n",
    "    add_y_array = []\n",
    "    for _ in tqdm(range(N), disable=not verbose):\n",
    "        tmp = []\n",
    "        for x, y in zip(data, label):\n",
    "            noise_rate = np.random.rand() * np.random.randint(0, 10)\n",
    "            shift_rate = np.random.randint(1, 50)\n",
    "\n",
    "            # Add Noise\n",
    "            tmp.append(ada.add_noise(x, noise_rate))\n",
    "            add_y_array.append(y)\n",
    "\n",
    "            tmp.append(ada.shift(x, shift_rate))\n",
    "            add_y_array.append(y)\n",
    "\n",
    "        data = np.vstack([data, tmp])\n",
    "    label = np.append(label, add_y_array)\n",
    "\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3cf7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(X, y, X_test):\n",
    "    p = np.random.permutation(np.arange(len(X)))\n",
    "    X_ = X[p]\n",
    "    y_ = y[p]\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_, y_)\n",
    "    X_train, y_train = augment(X_train, y_train, N=10)\n",
    "\n",
    "    model = build(X_train.shape)\n",
    "    es_cb = EarlyStopping(monitor=\"val_auc\", mode=\"auto\", patience=10)\n",
    "    model.fit(X_train, y_train, epochs=2, batch_size=32, validation_data=(X_valid, y_valid), callbacks=[es_cb],\n",
    "              verbose=True)\n",
    "\n",
    "    predict = model.predict(X_test)\n",
    "    del X_, y_, p, X_train, X_valid, y_train, y_valid, model;\n",
    "    gc.collect()\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a9f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "554/554 [==============================] - 1589s 3s/step - loss: 0.7585 - auc: 0.5262 - val_loss: 0.7000 - val_auc: 0.5155\n",
      "Epoch 2/2\n",
      "554/554 [==============================] - 1865s 3s/step - loss: 0.6938 - auc: 0.5678 - val_loss: 0.6816 - val_auc: 0.6007\n",
      "55/55 [==============================] - 166s 3s/step\n",
      "2 Fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "554/554 [==============================] - 25448s 46s/step - loss: 0.7397 - auc: 0.5396 - val_loss: 0.6922 - val_auc: 0.5854\n",
      "Epoch 2/2\n",
      "554/554 [==============================] - 1500s 3s/step - loss: 0.6887 - auc: 0.5782 - val_loss: 0.7018 - val_auc: 0.5882\n",
      "55/55 [==============================] - 51s 912ms/step\n",
      "3 Fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "554/554 [==============================] - 2265s 4s/step - loss: 0.7464 - auc: 0.5496 - val_loss: 0.6990 - val_auc: 0.5377\n",
      "Epoch 2/2\n",
      "554/554 [==============================] - 2380s 4s/step - loss: 0.6901 - auc: 0.5818 - val_loss: 0.7017 - val_auc: 0.5807\n",
      "55/55 [==============================] - 107s 2s/step\n",
      "4 Fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "554/554 [==============================] - 1725s 3s/step - loss: 0.7318 - auc: 0.5441 - val_loss: 0.7192 - val_auc: 0.5655\n",
      "Epoch 2/2\n",
      "554/554 [==============================] - 1712s 3s/step - loss: 0.6794 - auc: 0.6018 - val_loss: 0.6866 - val_auc: 0.5611\n",
      "55/55 [==============================] - 59s 1s/step\n",
      "5 Fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 35/554 [>.............................] - ETA: 29:40 - loss: 0.8335 - auc: 0.5160"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "K = len(normals_) // len(afs_)\n",
    "k_fold = KFold(n_splits=K, shuffle=True)\n",
    "\n",
    "oof = np.zeros(len(tests_))\n",
    "\n",
    "for i, ids in enumerate(k_fold.split(normals_)):\n",
    "    print(\"{} Fold\".format(i + 1))\n",
    "    gc.collect()\n",
    "\n",
    "    X = np.vstack([normals_[ids[1]], afs_])\n",
    "    y = np.append(np.zeros(len(ids[1])), np.ones(len(afs_)))\n",
    "\n",
    "    oof += fit_and_predict(X, y, tests_).flatten()\n",
    "\n",
    "oof /= K\n",
    "\n",
    "df_submit = pd.read_csv(os.path.join(dump_path, \"sampleSubmission.csv\"))\n",
    "df_submit[\"af\"] = oof\n",
    "df_submit.to_csv(\"submit_Keras.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3cd3293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RGB(r, g, b):\n",
    "    r = hex(r)[2:]\n",
    "    g = hex(g)[2:]\n",
    "    b = hex(b)[2:]\n",
    "    return r+g+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3950523c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'68f8f'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RGB(6, 143, 143)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0268b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
